from datetime import datetime
from time import sleep

from dotenv import load_dotenv
from pocketflow import Node

from agent.tools.parser import analyze_site
from agent.tools.search import search_web
from agent.tools.crawler import NewsCrawler
from agent.utils import call_llm
import yaml

load_dotenv()
__all__ = ["DecideAction", "SearchWeb", ]

total_links_count = 0


class DecideAction(Node):
    def prep(self, shared):
        """å‡†å¤‡ä¸Šä¸‹æ–‡å’Œé—®é¢˜ï¼Œç”¨äºå†³ç­–è¿‡ç¨‹ã€‚

        å‚æ•°:
            shared (dict): å…±äº«å­˜å‚¨ï¼ŒåŒ…å«ä¸Šä¸‹æ–‡å’Œé—®é¢˜ã€‚

        è¿”å›:
            tuple: åŒ…å«é—®é¢˜å’Œä¸Šä¸‹æ–‡çš„å…ƒç»„ã€‚
        """
        # è·å–å½“å‰ä¸Šä¸‹æ–‡ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼Œåˆ™é»˜è®¤ä¸ºâ€œæ— å…ˆå‰æœç´¢â€ï¼‰
        context = shared.get("context", "æ— å…ˆå‰æœç´¢")
        # ä»å…±äº«å­˜å‚¨ä¸­è·å–é—®é¢˜
        hot_word = shared["hot_word"]
        links_count = shared.get("links_count", 0)
        relation_news = shared["relation_news"]
        search_volume = shared["search_volume"]
        search_growth_rate = shared["search_growth_rate"]
        search_active_time = shared["search_active_time"]
        current_date = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        shared["current_date"] = current_date
        logger = shared["logger"]
        language = shared["language"]
        # è¿”å›é—®é¢˜å’Œä¸Šä¸‹æ–‡ï¼Œä¾› exec æ­¥éª¤ä½¿ç”¨
        return current_date, hot_word, search_volume, search_growth_rate, search_active_time, context, relation_news, links_count, language, logger

    def exec(self, inputs):
        """è°ƒç”¨ LLM å†³å®šæ˜¯æœç´¢è¿˜æ˜¯å›ç­”ã€‚"""
        current_date, hot_word, search_volume, search_growth_rate, search_active_time, context, relation_news, links_count, language, logger = inputs
        logger.info(f"ä»£ç†æ­£åœ¨å†³å®šä¸‹ä¸€æ­¥æ“ä½œ...")
        hot_word=hot_word.split("-",1)[1]
        desc = f"æ­¤çƒ­è¯ä»{search_active_time}å¼€å§‹æœç´¢æ´»è·ƒ,æœç´¢é‡ä¸Šå‡{search_growth_rate},æœç´¢æ€»é‡è¾¾åˆ°{search_volume}"
        # åˆ›å»ºä¸€ä¸ªæç¤ºï¼Œå¸®åŠ© LLM å†³å®šä¸‹ä¸€æ­¥æ“ä½œï¼Œå¹¶ä½¿ç”¨é€‚å½“çš„ yaml æ ¼å¼
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªå¯ä»¥æœç´¢ç½‘ç»œçš„çƒ­ç‚¹æ–°é—»æ·±åº¦æœç´¢åŠ©æ‰‹
ç°åœ¨ç»™ä½ ä¸€ä¸ªæ—¶ä¸‹ç½‘ç»œæµè¡Œçƒ­è¯ï¼Œä½ éœ€è¦å‚è€ƒæŸ¥è¯¢ç»´åº¦ã€å…ˆå‰çš„ç ”ç©¶è¿›è¡Œæ·±åº¦æœç´¢ï¼Œæ·±åº¦æ€è€ƒå¹¶ç†è§£è¯¥çƒ­è¯å¯¹åº”çš„å™äº‹å†…å®¹ã€‚
ä½¿ç”¨{language}å›ç­”
### æŸ¥è¯¢ç»´åº¦

- äº‹ä»¶åŸºæœ¬ä¿¡æ¯ : ç¡®è®¤çƒ­è¯å¯¹åº”çš„å…·ä½“äº‹ä»¶ã€æ—¶é—´ã€åœ°ç‚¹ã€ä¸»è¦äººç‰©
- äº‹ä»¶å‘å±•è„‰ç»œ : äº‹ä»¶èµ·å› ã€å…³é”®èŠ‚ç‚¹ã€æœ€æ–°è¿›å±•
- ç¤¾ä¼šå½±å“èŒƒå›´ : å—ä¼—ç¾¤ä½“ã€åœ°åŸŸå½±å“ã€è¡Œä¸šå½±å“
- äº‰è®®ç„¦ç‚¹ : å„æ–¹è§‚ç‚¹åˆ†æ­§ã€äº‰è®ºæ ¸å¿ƒé—®é¢˜
- å®˜æ–¹å›åº” : ç›¸å…³æƒå¨æœºæ„/äººç‰©çš„æ­£å¼è¡¨æ€
- å…³è”äº‹ä»¶ : ä¸æ­¤çƒ­ç‚¹ç›¸å…³çš„å†å²/å¹¶è¡Œäº‹ä»¶

å¹¶éæ‰€æœ‰æŸ¥è¯¢æ¡ä»¶éƒ½éœ€æ»¡è¶³ï¼Œå¯ä½¿ç”¨ä¼˜å…ˆçº§è¿›è¡Œæ’åº
æŸ¥è¯¢ä¼˜å…ˆçº§ï¼šäº‹ä»¶åŸºæœ¬ä¿¡æ¯>äº‹ä»¶å‘å±•è„‰ç»œ>ç¤¾ä¼šå½±å“èŒƒå›´>äº‰è®®ç„¦ç‚¹>å®˜æ–¹å›åº”>å…³è”äº‹ä»¶

## ä¸Šä¸‹æ–‡
- å½“å‰æ—¶é—´: {current_date}
- æ—¶ä¸‹æµè¡Œçƒ­è¯: {hot_word}
{desc}
- ç›¸å…³æ–°é—»æŠ¥å¯¼æ ‡é¢˜ï¼š

{relation_news}

- å…ˆå‰çš„ç ”ç©¶,æ€»è®¡ä¸º{links_count}æ¡,å…·ä½“å¦‚ä¸‹ï¼š

{context}

## æ“ä½œç©ºé—´
[1] search
  æè¿°: åœ¨ç½‘ç»œä¸ŠæŸ¥æ‰¾æ›´å¤šä¿¡æ¯
  å‚æ•°:
    - query (str): æœç´¢å†…å®¹

[2] answer
  æè¿°: ç”¨å½“å‰çŸ¥è¯†å›ç­”é—®é¢˜
  å‚æ•°:
    - answer (str): é—®é¢˜çš„æœ€ç»ˆå›ç­”

### ä¸‹ä¸€æ­¥æ“ä½œ
æ ¹æ®ä¸Šä¸‹æ–‡ã€æŸ¥è¯¢ç»´åº¦å’Œå¯ç”¨æ“ä½œå†³å®šä¸‹ä¸€æ­¥æ“ä½œã€‚
é‡è¦ï¼šè¯·ç¡®ä¿ï¼š
å¦‚å…ˆå‰çš„ç ”ç©¶ï¼Œæ€»è®¡å¤§äº6æ¡ï¼Œåˆ™ç»“åˆå·²æœ‰çš„ç ”ç©¶è¿›è¡Œå›ç­”æ“ä½œï¼Œä¸å†è¿›è¡Œæ·±åº¦æœç´¢ï¼Œ

è¯·ä»¥ä»¥ä¸‹æ ¼å¼è¿”å›ä½ çš„å“åº”ï¼š

```yaml
thinking: |
    <ä½ çš„é€æ­¥æ¨ç†è¿‡ç¨‹>
action: search OR answer
reason: <ä¸ºä»€ä¹ˆé€‰æ‹©è¿™ä¸ªæ“ä½œ>
answer: |
    <å¦‚æœæ“ä½œæ˜¯å›ç­”>
search_query: |
    <å…·ä½“çš„æœç´¢æŸ¥è¯¢å¦‚æœæ“ä½œæ˜¯æœç´¢>
```
é‡è¦ï¼šè¯·ç¡®ä¿ï¼š

å¦‚å…ˆå‰çš„ç ”ç©¶ï¼Œæ€»è®¡å¤§äº6æ¡ï¼Œåˆ™ç»“åˆå·²æœ‰çš„ç ”ç©¶è¿›è¡Œå›ç­”æ“ä½œï¼Œä¸å†è¿›è¡Œæ·±åº¦æœç´¢ï¼Œ
1. ä½¿ç”¨|å­—ç¬¦è¡¨ç¤ºå¤šè¡Œæ–‡æœ¬å­—æ®µ
2. å¤šè¡Œå­—æ®µä½¿ç”¨ç¼©è¿›ï¼ˆ4ä¸ªç©ºæ ¼ï¼‰
3. å•è¡Œå­—æ®µä¸ä½¿ç”¨|å­—ç¬¦
4. ä¸å…è®¸ç›´æ¥åœ¨é”®ååµŒå¥—å¦ä¸€ä¸ªé”®ï¼ˆå¦‚ answer: search_query:)
5. éé”®å€¼å¯¹ä¸å…è®¸éšæ„ä½¿ç”¨å†’å·: 
"""
        # è°ƒç”¨ LLM è¿›è¡Œå†³ç­–
        response, success = call_llm(prompt, logger)
        if not success:
            logger.error("LLM å“åº”å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä½ çš„å“åº”æ ¼å¼ã€‚")
            return {"action": "finish", "reason": "LLM å“åº”å¤±è´¥"}

        # è§£æå“åº”ä»¥è·å–å†³ç­–
        if "```yaml" not in response:
            logger.error("LLM å“åº”æ ¼å¼ä¸æ­£ç¡®ï¼Œè¯·æ£€æŸ¥ä½ çš„å“åº”æ ¼å¼ã€‚")
            return {"action": "finish", "reason": "LLM å“åº”æ ¼å¼ä¸æ­£ç¡®"}
        try:
            yaml_str = response.replace("\"", "").replace("\'", "").split("```yaml")[1].split("```")[0].strip()
            logger.info(f"LLM å“åº”: {yaml_str}")
            decision = yaml.safe_load(yaml_str)
        except Exception as e:
            return {"action": "finish", "reason": "LLM å“åº”æ ¼å¼ä¸æ­£ç¡®"}

        return decision

    def post(self, shared, prep_res, exec_res):
        """ä¿å­˜å†³ç­–å¹¶ç¡®å®šæµç¨‹ä¸­çš„ä¸‹ä¸€æ­¥ã€‚"""
        # å¦‚æœ LLM å†³å®šæœç´¢ï¼Œåˆ™ä¿å­˜æœç´¢æŸ¥è¯¢
        logger = shared["logger"]
        if exec_res["action"].strip() == "search":
            shared["search_query"] = exec_res["search_query"].strip()
            logger.info(f"ğŸ” ä»£ç†å†³å®šæœç´¢: {exec_res['search_query']}")
        else:
            shared["context"] = exec_res["answer"].strip()
            logger.info(f"ğŸ’¡ ä»£ç†å†³å®šå›ç­”é—®é¢˜")
            global total_links_count
            total_links_count = 0

        # è¿”å›æ“ä½œä»¥ç¡®å®šæµç¨‹ä¸­çš„ä¸‹ä¸€ä¸ªèŠ‚ç‚¹
        return exec_res["action"].strip()


class SearchWeb(Node):
    def prep(self, shared):
        """ä»å…±äº«å­˜å‚¨ä¸­è·å–æœç´¢æŸ¥è¯¢ã€‚"""
        return shared["search_query"], shared["hot_word_path"], shared["language"], shared["logger"]

    def exec(self, inputs):
        """æœç´¢ç½‘ç»œä¸Šçš„ç»™å®šæŸ¥è¯¢ã€‚"""
        # è°ƒç”¨æœç´¢å®ç”¨å‡½æ•°
        global total_links_count  # å£°æ˜ä½¿ç”¨å…¨å±€å˜é‡
        search_query, hot_word_path, language, logger = inputs
        logger.info(f"ğŸŒ åœ¨ç½‘ç»œä¸Šæœç´¢: {search_query}")
        sleep(5)
        _, results_dict = search_web(search_query, hot_word_path, logger)
        analyzed_results = []
        if results_dict is None:
            logger.info(f"ğŸŒ æ·±åº¦æœç´¢å¤±è´¥ã€‚")
            return {"action": "finish", "reason": "æœç´¢å¤±è´¥"}
        for i in results_dict:
            title = i['title']
            snippet = i['snippet']
            link = i['link']

            logger.info(f"ğŸŒ å¯¹æœç´¢çš„å†…å®¹è¿›é¡¹æ·±åº¦æ‰«æ")
            logger.info(f"ğŸŒ æ ‡é¢˜:{title}")
            logger.info(f"ğŸŒ æ‘˜è¦:{snippet}")

            logger.info(f"ğŸŒ æºé“¾æ¥:{link}")

            try:
                crawler = NewsCrawler(link)
                crawl_content = crawler.extract_information()
                if crawl_content['text'] != '':
                    crawl_content_analyze = analyze_site(crawl_content, logger, language)
                else:
                    logger.info(f"ğŸŒ æ·±åº¦æœç´¢ä¿¡æ¯ä¸ºç©ºæˆ–è€…æ˜¯äºŒè¿›åˆ¶è§†é¢‘")
                    crawl_content_analyze = {}
            except Exception as e:
                analyzed_results.append({
                    "results": {},
                    "title": title,
                    "url": link,
                    'snippet': snippet
                })
                logger.error(f"æ·±åº¦æœç´¢å¤±è´¥: {e}")
                continue
            analyzed_results.append({
                "results": crawl_content_analyze,
                "title": title,
                "url": link,
                'snippet': snippet
            })

        results = []
        for ret in analyzed_results:
            print(ret)
            if ret["results"] == {}:
                summary = 'æ— '
                title = ret.get('title', 'æ— ')
            else:
                content = ret["results"]
                summary = content['analysis']['summary'].replace('\n', '')
                title = content['analysis']['title'].replace('\n', '')
            total_links_count += 1
            result = (
                # f"æ ‡é¢˜ï¼š{content.get('title', 'æ— ')}\n" +
                    f"ğŸŒ æŠ¥é“{total_links_count}: {title}\n" +
                    f"é“¾æ¥ï¼š{ret.get('url', 'æ— ')}\n" +
                    # f"ç±»å‹ï¼š{content['analysis']['content_type']}\n" +
                    # f"è¯é¢˜ï¼š{','.join(content['analysis']['topics'])}\n" +
                    f" æ‘˜è¦-1ï¼š{summary}\n"
                    f" æ‘˜è¦-2ï¼š{ret.get('snippet', 'æ— ')}\n"
            )
            results.append(result)
            # ç»Ÿè®¡é“¾æ¥æ•°é‡
        # print(results)
        logger.info(f"âœ… å½“å‰å·²é‡‡é›†é“¾æ¥æ€»æ•°: {total_links_count}")

        return '\n'.join(results), total_links_count

    def post(self, shared, prep_res, exec_res):
        """ä¿å­˜æœç´¢ç»“æœå¹¶è¿”å›å†³ç­–èŠ‚ç‚¹ã€‚"""
        # å°†æœç´¢ç»“æœæ·»åŠ åˆ°å…±äº«å­˜å‚¨ä¸­çš„ä¸Šä¸‹æ–‡ä¸­
        results, links_count = exec_res
        previous = shared.get("context", "")
        search_history_previous = shared.get("search_history", "").strip()
        # æœç´¢è®°å¿†åŠŸèƒ½
        shared["context"] = previous + "\n\næœç´¢æ¡ä»¶: " + shared[
            "search_query"] + "\næœç´¢ç»“æœ(å¤šæ¡):\n " + results.strip()
        print("ğŸ“š æœç´¢ç»“æœ: " + results)
        shared["search_history"] = search_history_previous.strip() + results.strip()
        logger = shared["logger"]
        shared["links_count"] = links_count
        logger.info(f"ğŸ“š æ‰¾åˆ°ä¿¡æ¯ï¼Œåˆ†æç»“æœ...")

        # æœç´¢åå§‹ç»ˆè¿”å›å†³ç­–èŠ‚ç‚¹
        return "decide"


if __name__ == "__main__":
    import re

    response = """
```yaml
highlights:
  - title: ã€Šå®ä¹ å®Œç¾ã€‹å®‰å¨œÂ·åæ™®å…¬å¼€æ–°æ‹æƒ…ï¼Œç”œèœœäº’åŠ¨å¼•å‘å…³æ³¨
    summary: ç¾å›½æ¼”å‘˜å®‰å¨œÂ·åæ™®ï¼ˆAnna Campï¼‰ç¡®è®¤äº†ä¸é€ å‹å¸ˆJade Whipkeyçš„æ‹çˆ±å…³ç³»ï¼Œå¥¹åœ¨Instagramä¸Šåˆ†äº«äº†ä¸¤äººç”œèœœçš„çº¦ä¼šç…§ç‰‡ï¼Œå¹¶é…æœ‰çˆ±å¿ƒè¡¨æƒ…ã€‚Jade Whipkeyçš„å›åº”â€œå¥¹çš„ç¬‘å®¹æ˜¯è¯—â€ä¹Ÿè¿›ä¸€æ­¥ç¡®è®¤äº†å…³ç³»çš„ç”œèœœã€‚æ­¤æ¬¡å…¬å¼€æ‹æƒ…å¼•å‘äº†ç²‰ä¸çš„ç¥ç¦å’Œå…³æ³¨ã€‚
    link: "https://www.sohu.com/a/6726511775362662839"
  - title: ã€Šå®ä¹ å®Œç¾ã€‹å¥³æ¼”å‘˜å®‰å¨œÂ·åæ™®æ‹çˆ±äº†ï¼Ÿç”œèœœåˆå½±ç§€å‡ºæ–°æ‹æƒ…
    summary: æ¼”å‘˜å®‰å¨œÂ·åæ™®ï¼ˆAnna Campï¼‰å…¬å¼€äº†ä¸é€ å‹å¸ˆJade Whipkeyçš„æ‹æƒ…ï¼Œå¥¹åœ¨Instagramä¸Šåˆ†äº«äº†ä¸Jade Whipkeyçš„åˆå½±ï¼Œç”œèœœäº’åŠ¨å¼•å‘äº†ç½‘ç»œçƒ­è®®ã€‚å¥¹æ­¤å‰æ›¾ä¸æ¼”å‘˜Sylar Astinç»“å©šï¼Œç¦»å©šè¿‘å…­å¹´ï¼Œè¿™æ¬¡æ˜¯å¥¹ç¦»å©šåçš„é¦–æ¬¡å…¬å¼€æ‹æƒ…ã€‚
    link: "https://news.caijing.com.cn/20240120/1234648653.html"
```
    """

    yaml_str = response.split("```yaml")[1].split("```")[
        0].strip()
    # æ’å…¥æ¢è¡Œç¬¦ï¼Œå¼ºåˆ¶æ¯è¡Œä¸€ä¸ªå­—æ®µ
    yaml_str = re.sub(r":(\S)", r": \1", yaml_str)
    # å¼ºåˆ¶ä¸º YAML æ ‡è®°å­—æ®µæ·»åŠ æ¢è¡Œ
    yaml_str = re.sub(r'(highlights:|chinese:|output:)', r'\n\1', yaml_str)

    print(f"LLM å“åº”: {yaml_str}")
    decision = yaml.safe_load(yaml_str)
